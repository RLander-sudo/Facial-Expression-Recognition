{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4005d4d5-d638-4e3f-932c-48f2236153f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shapes:\n",
      "  Train: (25838, 48, 48, 1)\n",
      "  Validation: (2871, 48, 48, 1)\n",
      "  Test: (7178, 48, 48, 1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import pickle\n",
    "\n",
    "# Set paths\n",
    "train_dir = '../data/train'\n",
    "test_dir = '../data/test'\n",
    "img_size = 48  # Already 48x48, but you can change if needed\n",
    "\n",
    "# Class labels\n",
    "class_names = sorted(os.listdir(train_dir))  # ['angry', 'disgust', ...]\n",
    "\n",
    "def load_images_from_folder(folder_path, class_names):\n",
    "    \"\"\"\n",
    "    Load images from folders where each subfolder is a class\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    class_counts = {class_name: 0 for class_name in class_names}\n",
    "    \n",
    "    for label_idx, class_name in enumerate(class_names):\n",
    "        class_folder = os.path.join(folder_path, class_name)\n",
    "        for file in os.listdir(class_folder):\n",
    "            file_path = os.path.join(class_folder, file)\n",
    "            img = Image.open(file_path).convert('L')  # Convert to grayscale\n",
    "            img = img.resize((img_size, img_size))   # Resize\n",
    "            X.append(np.array(img))\n",
    "            y.append(label_idx)\n",
    "            class_counts[class_name] += 1\n",
    "    \n",
    "    return np.array(X), np.array(y), class_counts\n",
    "\n",
    "# Load training data\n",
    "X_train, y_train, train_counts = load_images_from_folder(train_dir, class_names)\n",
    "X_test, y_test, test_counts = load_images_from_folder(test_dir, class_names)\n",
    "\n",
    "# Normalize images\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Expand dimensions for CNN input: (samples, 48, 48, 1)\n",
    "X_train = np.expand_dims(X_train, -1)\n",
    "X_test = np.expand_dims(X_test, -1)\n",
    "\n",
    "# One-hot encode labels\n",
    "y_train_categorical = to_categorical(y_train, num_classes=len(class_names))\n",
    "y_test_categorical = to_categorical(y_test, num_classes=len(class_names))\n",
    "\n",
    "# Create validation set from training\n",
    "X_train, X_val, y_train_categorical, y_val = train_test_split(\n",
    "    X_train, y_train_categorical, test_size=0.1, stratify=y_train_categorical, random_state=42\n",
    ")\n",
    "\n",
    "# Calculate class weights for weighted loss function\n",
    "y_integers = np.argmax(y_train_categorical, axis=1)\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_integers),\n",
    "    y=y_integers\n",
    ")\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# Setup a data augmentation pipeline for minority classes\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "def balance_dataset_with_augmentation(X, y_categorical, target_samples_per_class=7000):\n",
    "    \"\"\"\n",
    "    Balance the dataset by generating augmented samples for minority classes\n",
    "    Args:\n",
    "        X: Input data, shape (samples, height, width, channels)\n",
    "        y_categorical: One-hot encoded labels\n",
    "        target_samples_per_class: Target number of samples per class\n",
    "    Returns:\n",
    "        X_balanced, y_balanced_categorical: Balanced dataset\n",
    "    \"\"\"\n",
    "    y_indices = np.argmax(y_categorical, axis=1)\n",
    "    X_balanced = []\n",
    "    y_balanced = []\n",
    "    \n",
    "    # Process each class\n",
    "    for class_idx in range(len(class_names)):\n",
    "        # Get samples of this class\n",
    "        indices = np.where(y_indices == class_idx)[0]\n",
    "        X_class = X[indices]\n",
    "        \n",
    "        # Add all original samples\n",
    "        X_balanced.extend(X_class)\n",
    "        y_balanced.extend([class_idx] * len(X_class))\n",
    "        \n",
    "        # If we need more samples, generate them with augmentation\n",
    "        if len(X_class) < target_samples_per_class:\n",
    "            samples_needed = target_samples_per_class - len(X_class)\n",
    "            \n",
    "            # Generate augmented samples\n",
    "            aug_samples = 0\n",
    "            batch_size = min(len(X_class), 32)  # Avoid too small batches\n",
    "            \n",
    "            # Configure generator for this class\n",
    "            augment_datagen = datagen.flow(\n",
    "                X_class, \n",
    "                np.ones(len(X_class)),  # Dummy labels\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True\n",
    "            )\n",
    "            \n",
    "            while aug_samples < samples_needed:\n",
    "                batch_X, _ = next(augment_datagen)\n",
    "                batch_size_actual = len(batch_X)\n",
    "                to_add = min(batch_size_actual, samples_needed - aug_samples)\n",
    "                \n",
    "                X_balanced.extend(batch_X[:to_add])\n",
    "                y_balanced.extend([class_idx] * to_add)\n",
    "                aug_samples += to_add\n",
    "    \n",
    "    # Convert to arrays and shuffle\n",
    "    X_balanced = np.array(X_balanced)\n",
    "    y_balanced = np.array(y_balanced)\n",
    "    \n",
    "    # Shuffle\n",
    "    indices = np.arange(len(X_balanced))\n",
    "    np.random.shuffle(indices)\n",
    "    X_balanced = X_balanced[indices]\n",
    "    y_balanced = y_balanced[indices]\n",
    "    \n",
    "    # Convert labels back to one-hot encoding\n",
    "    y_balanced_categorical = to_categorical(y_balanced, num_classes=len(class_names))\n",
    "    \n",
    "    return X_balanced, y_balanced_categorical\n",
    "\n",
    "# Uncomment the following line to apply data augmentation balancing\n",
    "# X_train_balanced, y_train_balanced = balance_dataset_with_augmentation(X_train, y_train_categorical)\n",
    "\n",
    "# Print shapes only\n",
    "print(\"Dataset shapes:\")\n",
    "print(f\"  Train: {X_train.shape}\")\n",
    "print(f\"  Validation: {X_val.shape}\")\n",
    "print(f\"  Test: {X_test.shape}\")\n",
    "\n",
    "np.save('../data/X_train.npy', X_train)\n",
    "np.save('../data/X_val.npy', X_val)\n",
    "np.save('../data/y_train.npy', y_train_categorical)\n",
    "np.save('../data/y_val.npy', y_val)\n",
    "np.save('../data/X_test.npy', X_test)\n",
    "np.save('../data/y_test.npy', y_test_categorical)\n",
    "\n",
    "with open('../data/class_weight.pkl', 'wb') as f:\n",
    "    pickle.dump(class_weight_dict, f)\n",
    "with open('../data/class_names.pkl', 'wb') as f:\n",
    "    pickle.dump(class_names, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e7f374-65dd-45e3-8717-5be1b6ffe095",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:myEnv]",
   "language": "python",
   "name": "conda-env-myEnv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
